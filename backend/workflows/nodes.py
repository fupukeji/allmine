"""
æŠ¥å‘Šç”Ÿæˆå·¥ä½œæµèŠ‚ç‚¹å®ç°
æ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£å·¥ä½œæµä¸­çš„ä¸€ä¸ªå…·ä½“æ­¥éª¤
"""
import logging
import json
from datetime import datetime
from typing import Dict, Any
from workflows.state import ReportWorkflowState


logger = logging.getLogger(__name__)


def _save_workflow_trace_realtime(state: ReportWorkflowState):
    """
    å®æ—¶ä¿å­˜å·¥ä½œæµè½¨è¿¹åˆ°æ•°æ®åº“
    åœ¨æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåè°ƒç”¨ï¼Œå®ç°å®æ—¶æ›´æ–°
    """
    from models.ai_report import AIReport
    from database import db
    
    try:
        task_context = state.get("task_context", {})
        report_id = task_context.get("report_id")
        
        if not report_id:
            return
        
        report = AIReport.query.get(report_id)
        if report:
            # ä¿å­˜å·¥ä½œæµè½¨è¿¹
            report.execution_path = json.dumps(state.get("execution_path", []), ensure_ascii=False)
            report.workflow_metadata = json.dumps({
                "agent_decisions": state.get("agent_decisions", []),
                "quality_score": state.get("quality_score"),
                "retry_count": state.get("retry_count", 0),
                "start_time": state.get("start_time"),
                "end_time": state.get("end_time")
            }, ensure_ascii=False)
            db.session.commit()
            
            node_count = len(state.get('execution_path', []))
            last_node = state['execution_path'][-1]['node'] if state.get('execution_path') else 'unknown'
            logger.info(f"ğŸ’¾ [å®æ—¶ä¿å­˜] æŠ¥å‘ŠID: {report_id} | èŠ‚ç‚¹æ•°: {node_count} | æœ€æ–°èŠ‚ç‚¹: {last_node}")
    except Exception as e:
        logger.warning(f"âš ï¸ [å®æ—¶ä¿å­˜] å¤±è´¥: {str(e)}")
        # ä¸å½±å“ä¸»æµç¨‹


async def init_task_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N1: åˆå§‹åŒ–ä»»åŠ¡èŠ‚ç‚¹
    - è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡
    - åˆå§‹åŒ–å·¥ä½œæµçŠ¶æ€
    - è®°å½•å¼€å§‹æ—¶é—´
    """
    task_context = state["task_context"]
    report_id = task_context.get("report_id")
    
    logger.info(f"ğŸš€ [N1-ä»»åŠ¡åˆå§‹åŒ–] å¼€å§‹ - æŠ¥å‘ŠID: {report_id}")
    
    # æ›´æ–°çŠ¶æ€
    state["start_time"] = datetime.utcnow().isoformat()
    state["retry_count"] = 0
    state["max_retries"] = 3
    state["agent_decisions"] = []
    state["execution_path"] = [{
        "node": "init_task",
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed"
    }]
    
    logger.info(f"âœ… [N1-ä»»åŠ¡åˆå§‹åŒ–] å®Œæˆ")
    
    # å®æ—¶ä¿å­˜
    _save_workflow_trace_realtime(state)
    
    return state


async def collect_data_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N2: æ•°æ®é‡‡é›†èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰
    - æŸ¥è¯¢å›ºå®šèµ„äº§æ•°æ®ï¼ˆå«æŠ˜æ—§æ˜ç»†ï¼‰
    - æŸ¥è¯¢è™šæ‹Ÿèµ„äº§æ•°æ®ï¼ˆå«æ—¶é—´ä»·å€¼åˆ†æï¼‰
    - æŸ¥è¯¢æ”¶å…¥æ•°æ®ï¼ˆå«ROIè®¡ç®—ï¼‰
    - æŸ¥è¯¢åˆ†ç±»å±‚çº§ç»“æ„
    - æ„å»ºç»“æ„åŒ–æ•°æ® + æ™ºèƒ½æ´å¯Ÿ
    """
    from services.zhipu_service import ZhipuAiService
    from models.category import Category
    from datetime import datetime
    
    task_context = state["task_context"]
    user_id = task_context["user_id"]
    start_date = task_context["start_date"]
    end_date = task_context["end_date"]
    
    logger.info(f"ğŸ“Š [N2-æ•°æ®é‡‡é›†å¢å¼º] å¼€å§‹ - ç”¨æˆ·ID: {user_id}, æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
    
    try:
        # ä½¿ç”¨ç°æœ‰çš„æœåŠ¡æ–¹æ³•å‡†å¤‡åŸºç¡€æ•°æ®
        service = ZhipuAiService(api_token="dummy", model="dummy")  # ä»…ç”¨äºæ•°æ®æŸ¥è¯¢
        raw_data = service.prepare_asset_data(user_id, start_date, end_date)
        
        # ã€å¢å¼º1ã€‘è·å–åˆ†ç±»å±‚çº§ç»“æ„
        categories = Category.query.filter_by(user_id=user_id).all()
        category_hierarchy = []
        for cat in categories:
            category_hierarchy.append({
                'id': cat.id,
                'name': cat.name,
                'parent_id': cat.parent_id,
                'level': cat.get_level(),
                'full_path': cat.get_full_path(),
                'project_count': len(cat.projects)
            })
        
        # ã€å¢å¼º2ã€‘è®¡ç®—æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡
        insights = {
            # å›ºå®šèµ„äº§å¥åº·åº¦
            'fixed_asset_health': _calculate_fixed_asset_health(raw_data['fixed_assets']),
            # è™šæ‹Ÿèµ„äº§æ•ˆç‡è¯„çº§
            'virtual_asset_efficiency': _calculate_virtual_efficiency(raw_data['virtual_assets']),
            # æ”¶å…¥è´¨é‡è¯„åˆ†
            'income_quality': _calculate_income_quality(raw_data['fixed_assets']),
            # èµ„äº§é…ç½®å‡è¡¡åº¦
            'allocation_balance': _calculate_allocation_balance(raw_data),
            # åˆ†ç±»å±‚çº§æ•°æ®
            'category_hierarchy': category_hierarchy
        }
        
        state["raw_data"] = raw_data
        
        logger.info(f"âœ… [N2-æ•°æ®é‡‡é›†å¢å¼º] å®Œæˆ")
        logger.info(f"   - å›ºå®šèµ„äº§: {raw_data['fixed_assets']['total_assets']}é¡¹")
        logger.info(f"   - è™šæ‹Ÿèµ„äº§: {raw_data['virtual_assets']['total_projects']}é¡¹")
        logger.info(f"   - åˆ†ç±»å±‚çº§: {len(category_hierarchy)}ä¸ªåˆ†ç±»ï¼Œæœ€æ·±{max([c['level'] for c in category_hierarchy], default=0)}å±‚")
        logger.info(f"   - æ™ºèƒ½æ´å¯Ÿ: å¥åº·åº¦{insights['fixed_asset_health']:.1f}, æ•ˆç‡{insights['virtual_asset_efficiency']:.1f}")
        
        # å°†insightså•ç‹¬å­˜å‚¨ï¼Œä¸ä¿®æ”¹raw_dataç»“æ„
        state["intelligent_insights"] = insights
        
        state["execution_path"].append({
            "node": "collect_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "data_summary": {
                "fixed_assets_count": raw_data['fixed_assets']['total_assets'],
                "virtual_assets_count": raw_data['virtual_assets']['total_projects'],
                "category_count": len(category_hierarchy),
                "insights": insights
            }
        })
        
        # å®æ—¶ä¿å­˜
        _save_workflow_trace_realtime(state)
        
    except Exception as e:
        logger.error(f"âŒ [N2-æ•°æ®é‡‡é›†] å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        state["error_message"] = f"æ•°æ®é‡‡é›†å¤±è´¥: {str(e)}"
        state["execution_path"].append({
            "node": "collect_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


def _calculate_fixed_asset_health(fixed_data):
    """è®¡ç®—å›ºå®šèµ„äº§å¥åº·åº¦ï¼ˆ0-100ï¼‰"""
    if fixed_data['total_assets'] == 0:
        return 50.0  # æ— èµ„äº§é»˜è®¤ä¸­ç­‰
    
    # è€ƒè™‘å› ç´ ï¼šæŠ˜æ—§ç‡ã€æ”¶å…¥ç‡ã€ä½¿ç”¨ç‡
    health_score = 100.0
    
    # æŠ˜æ—§ç‡æƒ©ç½šï¼ˆæŠ˜æ—§ç‡è¶Šé«˜æ‰£åˆ†è¶Šå¤šï¼‰
    depreciation_penalty = min(40, fixed_data['depreciation_rate'] * 0.5)
    health_score -= depreciation_penalty
    
    # æ”¶å…¥ç‡åŠ åˆ†
    if fixed_data['total_current_value'] > 0:
        income_rate = (fixed_data['total_income'] / fixed_data['total_current_value']) * 100
        income_bonus = min(30, income_rate * 3)
        health_score += income_bonus
    
    # ä½¿ç”¨ç‡åŠ åˆ†
    status_stats = fixed_data.get('status_stats', {})
    if fixed_data['total_assets'] > 0:
        in_use = status_stats.get('ä½¿ç”¨ä¸­', 0)
        usage_rate = (in_use / fixed_data['total_assets']) * 100
        usage_bonus = min(20, usage_rate * 0.2)
        health_score += usage_bonus
    
    return round(max(0, min(100, health_score)), 1)


def _calculate_virtual_efficiency(virtual_data):
    """è®¡ç®—è™šæ‹Ÿèµ„äº§æ•ˆç‡ï¼ˆ0-100ï¼‰"""
    if virtual_data['total_projects'] == 0:
        return 50.0
    
    # åŸºäºåˆ©ç”¨ç‡å’Œæµªè´¹ç‡
    efficiency = virtual_data['utilization_rate'] - virtual_data['waste_rate'] * 2
    
    # å³å°†è¿‡æœŸé¡¹ç›®æ‰£åˆ†
    if virtual_data.get('expiring_soon'):
        expiring_count = len(virtual_data['expiring_soon'])
        efficiency -= min(20, expiring_count * 5)
    
    return round(max(0, min(100, efficiency)), 1)


def _calculate_income_quality(fixed_data):
    """è®¡ç®—æ”¶å…¥è´¨é‡ï¼ˆ0-100ï¼‰"""
    if fixed_data['total_current_value'] == 0:
        return 0.0
    
    # ROIä½œä¸ºä¸»è¦æŒ‡æ ‡
    roi = (fixed_data['total_income'] / fixed_data['total_current_value']) * 100
    
    # è½¬æ¢ä¸º0-100åˆ†æ•°
    # 10% ROI = 100åˆ†, 5% ROI = 50åˆ†, 0% ROI = 0åˆ†
    quality = roi * 10
    
    return round(max(0, min(100, quality)), 1)


def _calculate_allocation_balance(raw_data):
    """è®¡ç®—èµ„äº§é…ç½®å‡è¡¡åº¦ï¼ˆ0-100ï¼‰"""
    fixed_value = raw_data['fixed_assets']['total_current_value']
    virtual_value = raw_data['virtual_assets']['total_amount']
    total = fixed_value + virtual_value
    
    if total == 0:
        return 50.0
    
    # ç†æƒ³æ¯”ä¾‹ï¼šå›ºå®šèµ„äº§60-80%ï¼Œè™šæ‹Ÿèµ„äº§20-40%
    fixed_ratio = (fixed_value / total) * 100
    
    if 60 <= fixed_ratio <= 80:
        balance = 100  # å®Œç¾å‡è¡¡
    elif 50 <= fixed_ratio < 60 or 80 < fixed_ratio <= 90:
        balance = 80   # è‰¯å¥½
    elif 40 <= fixed_ratio < 50 or 90 < fixed_ratio <= 95:
        balance = 60   # ä¸€èˆ¬
    else:
        balance = 40   # å¤±è¡¡
    
    return round(balance, 1)


async def compress_data_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N3: æ•°æ®å‹ç¼©èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰
    - å°†ç»“æ„åŒ–æ•°æ®å‹ç¼©ä¸ºç®€æ´æ–‡æœ¬
    - èåˆæ™ºèƒ½æ´å¯ŸæŒ‡æ ‡
    - ä¸ºAIç”Ÿæˆåšå‡†å¤‡
    """
    from services.zhipu_service import ZhipuAiService
    
    logger.info(f"ğŸ—œï¸ [N3-æ•°æ®å‹ç¼©å¢å¼º] å¼€å§‹")
    
    try:
        raw_data = state["raw_data"]
        if not raw_data:
            raise Exception("åŸå§‹æ•°æ®ä¸ºç©º")
        
        service = ZhipuAiService(api_token="dummy", model="dummy")
        compressed_text = service._compress_data_to_text(raw_data)
        
        # ã€å¢å¼ºã€‘æ·»åŠ æ™ºèƒ½æ´å¯Ÿæ‘˜è¦
        insights = state.get("intelligent_insights")
        if insights:
            insights_text = "\n\nã€æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡ã€‘\n"
            insights_text += f"- ğŸŸ¢ å›ºå®šèµ„äº§å¥åº·åº¦: {insights['fixed_asset_health']:.1f}/100\n"
            insights_text += f"- âš¡ è™šæ‹Ÿèµ„äº§æ•ˆç‡: {insights['virtual_asset_efficiency']:.1f}/100\n"
            insights_text += f"- ğŸ’µ æ”¶å…¥è´¨é‡: {insights['income_quality']:.1f}/100\n"
            insights_text += f"- âš–ï¸ èµ„äº§é…ç½®å‡è¡¡åº¦: {insights['allocation_balance']:.1f}/100\n"
            
            # åˆ†ç±»å±‚çº§ä¿¡æ¯
            if 'category_hierarchy' in insights:
                max_level = max([c['level'] for c in insights['category_hierarchy']], default=0)
                top_categories = [c for c in insights['category_hierarchy'] if c['level'] == 0]
                insights_text += f"\n- ğŸ“‚ åˆ†ç±»ç»“æ„: {len(insights['category_hierarchy'])}ä¸ªåˆ†ç±»ï¼Œ{max_level+1}å±‚æ·±åº¦ï¼Œ{len(top_categories)}ä¸ªé¡¶çº§åˆ†ç±»"
                # æ˜¾ç¤ºå‰3ä¸ªæœ€æ´»è·ƒçš„åˆ†ç±»
                sorted_cats = sorted(insights['category_hierarchy'], key=lambda x: x['project_count'], reverse=True)[:3]
                for cat in sorted_cats:
                    insights_text += f"\n  â€¢ {cat['full_path']}: {cat['project_count']}ä¸ªé¡¹ç›®"
            
            compressed_text += insights_text
        
        state["compressed_text"] = compressed_text
        
        logger.info(f"âœ… [N3-æ•°æ®å‹ç¼©å¢å¼º] å®Œæˆ - å‹ç¼©åæ–‡æœ¬é•¿åº¦: {len(compressed_text)} å­—ç¬¦")
        logger.debug(f"å‹ç¼©åæ–‡æœ¬é¢„è§ˆ:\n{compressed_text[:500]}...")
        
        state["execution_path"].append({
            "node": "compress_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "text_length": len(compressed_text),
            "has_insights": insights is not None
        })
        
        # å®æ—¶ä¿å­˜
        _save_workflow_trace_realtime(state)
        
    except Exception as e:
        logger.error(f"âŒ [N3-æ•°æ®å‹ç¼©] å¤±è´¥: {str(e)}")
        state["error_message"] = f"æ•°æ®å‹ç¼©å¤±è´¥: {str(e)}"
        state["execution_path"].append({
            "node": "compress_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


async def agent_decide_comparison_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N4: Agentå†³ç­–-æ˜¯å¦éœ€è¦ä¸ŠæœŸå¯¹æ¯”
    - åˆ†ææŠ¥å‘Šç±»å‹å’Œæ—¶é—´è·¨åº¦
    - å†³å®šæ˜¯å¦æŸ¥è¯¢ä¸ŠæœŸæ•°æ®
    """
    logger.info(f"ğŸ¤– [N4-Agentå†³ç­–] å¼€å§‹ - åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸ŠæœŸå¯¹æ¯”")
    
    task_context = state["task_context"]
    report_type = task_context.get("report_type", "custom")
    
    # å†³ç­–é€»è¾‘ï¼šå‘¨æŠ¥ã€æœˆæŠ¥ã€å¹´æŠ¥éœ€è¦ä¸ŠæœŸå¯¹æ¯”
    need_comparison = report_type in ['weekly', 'monthly', 'yearly']
    
    decision = {
        "node": "agent_decide_comparison",
        "timestamp": datetime.utcnow().isoformat(),
        "decision": {
            "need_comparison": need_comparison,
            "reason": f"æŠ¥å‘Šç±»å‹ä¸º{report_type}ï¼Œ{'éœ€è¦' if need_comparison else 'æ— éœ€'}ä¸ŠæœŸå¯¹æ¯”",
            "next_node": "query_previous_data" if need_comparison else "generate_report"
        }
    }
    
    state["agent_decisions"].append(decision)
    state["execution_path"].append({
        "node": "agent_decide_comparison",
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed",
        "decision": decision["decision"]
    })
    
    logger.info(f"âœ… [N4-Agentå†³ç­–] å®Œæˆ - {'éœ€è¦' if need_comparison else 'æ— éœ€'}ä¸ŠæœŸå¯¹æ¯”")
    
    return state


async def query_previous_data_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N5: æŸ¥è¯¢ä¸ŠæœŸæ•°æ®èŠ‚ç‚¹
    - è®¡ç®—ä¸ŠæœŸæ—¶é—´èŒƒå›´
    - æŸ¥è¯¢ä¸ŠæœŸæ•°æ®
    - ç”Ÿæˆå¯¹æ¯”æ–‡æœ¬
    """
    from services.zhipu_service import ZhipuAiService
    
    logger.info(f"ğŸ“… [N5-æŸ¥è¯¢ä¸ŠæœŸ] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        user_id = task_context["user_id"]
        start_date = task_context["start_date"]
        end_date = task_context["end_date"]
        
        service = ZhipuAiService(api_token="dummy", model="dummy")
        previous_data = service._get_previous_period_data(user_id, start_date, end_date)
        
        if previous_data:
            state["previous_data"] = previous_data
            
            # ç”Ÿæˆå¯¹æ¯”æ–‡æœ¬
            comparison_text = service._generate_comparison_text(state["raw_data"], previous_data)
            state["comparison_text"] = comparison_text
            
            logger.info(f"âœ… [N5-æŸ¥è¯¢ä¸ŠæœŸ] å®Œæˆ - å·²ç”Ÿæˆå¯¹æ¯”åˆ†æ")
        else:
            logger.warning(f"âš ï¸ [N5-æŸ¥è¯¢ä¸ŠæœŸ] æœªæ‰¾åˆ°ä¸ŠæœŸæ•°æ®ï¼Œè·³è¿‡å¯¹æ¯”")
            state["comparison_text"] = ""
        
        state["execution_path"].append({
            "node": "query_previous_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "has_previous_data": previous_data is not None
        })
        
    except Exception as e:
        logger.warning(f"âš ï¸ [N5-æŸ¥è¯¢ä¸ŠæœŸ] å¼‚å¸¸: {str(e)}ï¼Œè·³è¿‡å¯¹æ¯”")
        state["comparison_text"] = ""
        state["execution_path"].append({
            "node": "query_previous_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "skipped",
            "error": str(e)
        })
    
    return state


async def ai_preanalysis_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N6: AIé¢„åˆ†æèŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰
    - å¯¹å‹ç¼©æ•°æ®è¿›è¡ŒAIé¢„åˆ†æ
    - ç”Ÿæˆæ´å¯Ÿæ–‡æœ¬
    """
    from services.zhipu_service import ZhipuAiService
    
    logger.info(f"ğŸ§  [N6-AIé¢„åˆ†æ] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        api_key = task_context.get("api_key")
        model = task_context.get("model", "glm-4-flash")
        
        if not api_key:
            logger.warning(f"âš ï¸ [N6-AIé¢„åˆ†æ] API Keyæœªé…ç½®ï¼Œè·³è¿‡é¢„åˆ†æ")
            state["ai_insights"] = ""
            state["execution_path"].append({
                "node": "ai_preanalysis",
                "timestamp": datetime.utcnow().isoformat(),
                "status": "skipped",
                "reason": "API Keyæœªé…ç½®"
            })
            return state
        
        service = ZhipuAiService(api_token=api_key, model=model)
        compressed_text = state["compressed_text"]
        
        # é»˜è®¤ç¦ç”¨AIæ´å¯Ÿä»¥èŠ‚çœAPIè°ƒç”¨ï¼ˆå¯é€šè¿‡é…ç½®å¯ç”¨ï¼‰
        enable_ai_insights = task_context.get("enable_ai_insights", False)
        ai_insights = service._preprocess_data_with_ai(compressed_text, enable_ai_insights=enable_ai_insights)
        
        state["ai_insights"] = ai_insights
        
        logger.info(f"âœ… [N6-AIé¢„åˆ†æ] å®Œæˆ - æ´å¯Ÿé•¿åº¦: {len(ai_insights)} å­—ç¬¦")
        
        state["execution_path"].append({
            "node": "ai_preanalysis",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "insights_length": len(ai_insights)
        })
        
    except Exception as e:
        logger.error(f"âŒ [N6-AIé¢„åˆ†æ] å¤±è´¥: {str(e)}ï¼Œç»§ç»­æ‰§è¡Œ")
        state["ai_insights"] = ""
        state["execution_path"].append({
            "node": "ai_preanalysis",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


async def generate_report_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N7: ç”ŸæˆæŠ¥å‘ŠèŠ‚ç‚¹
    - è°ƒç”¨AIç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    - è§£æJSONæ ¼å¼å†…å®¹
    """
    from services.zhipu_service import ZhipuAiService
    
    logger.info(f"ğŸ“ [N7-ç”ŸæˆæŠ¥å‘Š] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        api_key = task_context["api_key"]
        model = task_context.get("model", "glm-4-flash")
        report_type = task_context.get("report_type", "custom")
        
        service = ZhipuAiService(api_token=api_key, model=model)
        
        # æ ¹æ®æŠ¥å‘Šç±»å‹è°ƒç”¨å¯¹åº”çš„ç”Ÿæˆæ–¹æ³•
        user_id = task_context["user_id"]
        start_date = task_context["start_date"]
        end_date = task_context["end_date"]
        focus_areas = task_context.get("focus_areas", [])
        
        if report_type == 'weekly':
            content = service.generate_weekly_report(user_id, start_date, end_date)
        elif report_type == 'monthly':
            content = service.generate_monthly_report(user_id, start_date, end_date)
        elif report_type == 'yearly':
            content = service.generate_custom_report(
                user_id, start_date, end_date, 
                focus_areas or ['å¹´åº¦èµ„äº§å¢é•¿è¶‹åŠ¿', 'å¹´åº¦æ”¶ç›Šè¡¨ç°', 'èµ„äº§é…ç½®ä¼˜åŒ–']
            )
        else:  # custom
            content = service.generate_custom_report(user_id, start_date, end_date, focus_areas)
        
        state["report_content"] = content
        
        logger.info(f"âœ… [N7-ç”ŸæˆæŠ¥å‘Š] å®Œæˆ - å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        state["execution_path"].append({
            "node": "generate_report",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "content_length": len(content)
        })
        
    except Exception as e:
        logger.error(f"âŒ [N7-ç”ŸæˆæŠ¥å‘Š] å¤±è´¥: {str(e)}")
        state["error_message"] = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
        state["execution_path"].append({
            "node": "generate_report",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


async def evaluate_quality_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N8: è´¨é‡è¯„ä¼°èŠ‚ç‚¹
    - æ£€æŸ¥JSONæ ¼å¼
    - è¯„ä¼°å†…å®¹å®Œæ•´æ€§
    - è¯„ä¼°æ•°æ®å‡†ç¡®æ€§
    - è®¡ç®—è´¨é‡åˆ†æ•°
    """
    import json
    
    logger.info(f"ğŸ” [N8-è´¨é‡è¯„ä¼°] å¼€å§‹")
    
    try:
        report_content = state["report_content"]
        
        if not report_content:
            raise Exception("æŠ¥å‘Šå†…å®¹ä¸ºç©º")
        
        # å°è¯•è§£æJSON
        try:
            content_json = json.loads(report_content)
        except json.JSONDecodeError as e:
            raise Exception(f"JSONè§£æå¤±è´¥: {str(e)}")
        
        # è¯„ä¼°ç»´åº¦
        score = {
            "json_validity": 100,  # JSONæ ¼å¼æœ‰æ•ˆ
            "completeness": 0,  # å†…å®¹å®Œæ•´æ€§
            "data_accuracy": 0,  # æ•°æ®å‡†ç¡®æ€§
            "total_score": 0
        }
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = [
            'executive_summary', 'key_conclusions', 'fixed_asset_analysis',
            'virtual_asset_analysis', 'income_performance', 'asset_allocation_review',
            'actionable_recommendations', 'risk_alerts', 'health_score', 'chart_data'
        ]
        
        present_fields = sum(1 for field in required_fields if field in content_json)
        score["completeness"] = int((present_fields / len(required_fields)) * 100)
        
        # æ£€æŸ¥æ•°æ®å¼•ç”¨ï¼ˆç®€å•æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å€¼ï¼‰
        content_str = str(content_json)
        has_numbers = any(char.isdigit() for char in content_str)
        score["data_accuracy"] = 100 if has_numbers else 0
        
        # è®¡ç®—æ€»åˆ†
        score["total_score"] = int(
            score["json_validity"] * 0.3 +
            score["completeness"] * 0.4 +
            score["data_accuracy"] * 0.3
        )
        
        state["quality_score"] = score
        
        # åˆ¤æ–­æ˜¯å¦åˆæ ¼ï¼ˆæ€»åˆ†â‰¥80ï¼‰
        if score["total_score"] >= 60:
            state["evaluation_result"] = "pass"
            logger.info(f"âœ… [N8-è´¨é‡è¯„ä¼°] å®Œæˆ - è¯„åˆ†: {score['total_score']:.1f}/100ï¼Œåˆæ ¼")
        elif state["retry_count"] < state["max_retries"]:
            state["evaluation_result"] = "retry"
            logger.warning(f"âš ï¸ [N8-è´¨é‡è¯„ä¼°] å®Œæˆ - è¯„åˆ†: {score['total_score']:.1f}/100ï¼Œéœ€è¦é‡è¯•")
        else:
            state["evaluation_result"] = "fail"
            logger.error(f"âŒ [N8-è´¨é‡è¯„ä¼°] å®Œæˆ - è¯„åˆ†: {score['total_score']:.1f}/100ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
        
        state["execution_path"].append({
            "node": "evaluate_quality",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "quality_score": score,
            "evaluation_result": state["evaluation_result"]
        })
        
    except Exception as e:
        logger.error(f"âŒ [N8-è´¨é‡è¯„ä¼°] å¤±è´¥: {str(e)}")
        state["evaluation_result"] = "retry" if state["retry_count"] < state["max_retries"] else "fail"
        state["execution_path"].append({
            "node": "evaluate_quality",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e),
            "evaluation_result": state["evaluation_result"]
        })
    
    return state


async def save_report_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N9: ä¿å­˜æŠ¥å‘ŠèŠ‚ç‚¹
    - è§£ææŠ¥å‘Šå†…å®¹æå–æ‘˜è¦
    - æ›´æ–°æ•°æ®åº“æŠ¥å‘ŠçŠ¶æ€
    """
    import json
    from models.ai_report import AIReport
    from database import db
    
    logger.info(f"ğŸ’¾ [N9-ä¿å­˜æŠ¥å‘Š] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        report_id = task_context["report_id"]
        
        report = AIReport.query.get(report_id)
        if not report:
            raise Exception(f"æŠ¥å‘Šä¸å­˜åœ¨: {report_id}")
        
        # æå–æ‘˜è¦
        content = state["report_content"]
        if not content:
            raise Exception("æŠ¥å‘Šå†…å®¹ä¸ºç©º")
        
        try:
            content_json = json.loads(content)
            # æ–°æ ¼å¼ï¼šexecutive_summary æ˜¯å¯¹è±¡
            if 'executive_summary' in content_json:
                exec_summary = content_json['executive_summary']
                if isinstance(exec_summary, dict):
                    summary = exec_summary.get('content', exec_summary.get('title', 'æŠ¥å‘Šå·²ç”Ÿæˆ'))
                else:
                    summary = str(exec_summary)
            # æ—§æ ¼å¼ï¼šperiod_summary æ˜¯å­—ç¬¦ä¸²
            elif 'period_summary' in content_json:
                summary = str(content_json['period_summary'])
            else:
                summary = "æŠ¥å‘Šå·²ç”Ÿæˆ"
            
            summary = str(summary)[:500] if summary else "æŠ¥å‘Šå·²ç”Ÿæˆ"
        except Exception as e:
            logger.warning(f"âš ï¸ æ‘˜è¦æå–å¤±è´¥: {e}")
            summary = "æŠ¥å‘Šå·²ç”Ÿæˆ"
        
        # ã€å¢å¼ºã€‘æ·»åŠ æ™ºèƒ½æ´å¯Ÿåˆ°æŠ¥å‘Šå†…å®¹
        intelligent_insights = state.get("intelligent_insights")
        if intelligent_insights:
            # å¦‚æœæ˜¯Markdownæ ¼å¼ï¼Œæ³¨å…¥æ™ºèƒ½æ´å¯Ÿ
            try:
                report_data = json.loads(content)
                if isinstance(report_data, dict):
                    report_data["intelligent_insights"] = intelligent_insights
                    content = json.dumps(report_data, ensure_ascii=False)
                    logger.info(f"âœ… å·²æ³¨å…¥æ™ºèƒ½æ´å¯Ÿåˆ°æŠ¥å‘Šå†…å®¹")
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä¿æŒåŸæ ·
                pass
        
        # æ›´æ–°æŠ¥å‘Š
        report.content = content
        report.summary = summary
        report.status = 'completed'
        report.generated_at = datetime.utcnow()
        
        # æŒä¹…åŒ–å·¥ä½œæµè½¨è¿¹ï¼ˆæ–°å¢ï¼‰
        report.execution_path = json.dumps(state.get("execution_path", []), ensure_ascii=False)
        report.workflow_metadata = json.dumps({
            "agent_decisions": state.get("agent_decisions", []),
            "quality_score": state.get("quality_score"),
            "retry_count": state.get("retry_count", 0),
            "start_time": state.get("start_time"),
            "end_time": state.get("end_time")
        }, ensure_ascii=False)
        
        db.session.commit()
        
        state["end_time"] = datetime.utcnow().isoformat()
        
        logger.info(f"âœ… [N9-ä¿å­˜æŠ¥å‘Š] å®Œæˆ - æŠ¥å‘ŠID: {report_id}")
        
        state["execution_path"].append({
            "node": "save_report",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed"
        })
        
    except Exception as e:
        logger.error(f"âŒ [N9-ä¿å­˜æŠ¥å‘Š] å¤±è´¥: {str(e)}")
        state["error_message"] = f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}"
        state["execution_path"].append({
            "node": "save_report",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


async def handle_retry_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N10: é‡è¯•å¤„ç†èŠ‚ç‚¹
    - å¢åŠ é‡è¯•è®¡æ•°
    - åˆ†æå¤±è´¥åŸå› 
    - è°ƒæ•´ç”Ÿæˆç­–ç•¥
    """
    logger.info(f"ğŸ”„ [N10-é‡è¯•å¤„ç†] å¼€å§‹")
    
    state["retry_count"] += 1
    current_retry = state["retry_count"]
    max_retries = state["max_retries"]
    
    logger.info(f"âš ï¸ [N10-é‡è¯•å¤„ç†] ç¬¬ {current_retry}/{max_retries} æ¬¡é‡è¯•")
    
    # è®°å½•é‡è¯•å†³ç­–
    decision = {
        "node": "handle_retry",
        "timestamp": datetime.utcnow().isoformat(),
        "decision": {
            "retry_count": current_retry,
            "max_retries": max_retries,
            "reason": "è´¨é‡è¯„ä¼°æœªé€šè¿‡ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ",
            "next_node": "generate_report"
        }
    }
    
    state["agent_decisions"].append(decision)
    state["execution_path"].append({
        "node": "handle_retry",
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed",
        "retry_count": current_retry
    })
    
    logger.info(f"âœ… [N10-é‡è¯•å¤„ç†] å®Œæˆ - å‡†å¤‡é‡æ–°ç”ŸæˆæŠ¥å‘Š")
    
    return state


async def handle_failure_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N11: å¤±è´¥å¤„ç†èŠ‚ç‚¹
    - è®°å½•å¤±è´¥åŸå› 
    - æ›´æ–°æŠ¥å‘ŠçŠ¶æ€ä¸ºå¤±è´¥
    - ä¿å­˜å·¥ä½œæµè½¨è¿¹
    """
    import json
    from models.ai_report import AIReport
    from database import db
    
    logger.error(f"âŒ [N11-å¤±è´¥å¤„ç†] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        report_id = task_context["report_id"]
        
        report = AIReport.query.get(report_id)
        if report:
            # ç¡®ä¿error_messageå­˜åœ¨
            if not state.get("error_message"):
                state["error_message"] = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°"
            
            error_msg = state["error_message"]
            report.status = 'failed'
            report.error_message = error_msg
            
            # ä¿å­˜å·¥ä½œæµè½¨è¿¹ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè¦ä¿å­˜ï¼‰
            report.execution_path = json.dumps(state.get("execution_path", []), ensure_ascii=False)
            report.workflow_metadata = json.dumps({
                "agent_decisions": state.get("agent_decisions", []),
                "quality_score": state.get("quality_score"),
                "retry_count": state.get("retry_count", 0),
                "start_time": state.get("start_time"),
                "end_time": state.get("end_time"),
                "error_message": error_msg
            }, ensure_ascii=False)
            
            db.session.commit()
            
            logger.error(f"âŒ [N11-å¤±è´¥å¤„ç†] å®Œæˆ - æŠ¥å‘ŠID: {report_id}, åŸå› : {error_msg}")
        
        state["end_time"] = datetime.utcnow().isoformat()
        state["execution_path"].append({
            "node": "handle_failure",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed"
        })
        
    except Exception as e:
        logger.error(f"âŒ [N11-å¤±è´¥å¤„ç†] å¼‚å¸¸: {str(e)}")
        state["execution_path"].append({
            "node": "handle_failure",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "error",
            "error": str(e)
        })
    
    return state
