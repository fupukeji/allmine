"""
æŠ¥å‘Šç”Ÿæˆå·¥ä½œæµèŠ‚ç‚¹å®ç°
æ¯ä¸ªèŠ‚ç‚¹è´Ÿè´£å·¥ä½œæµä¸­çš„ä¸€ä¸ªå…·ä½“æ­¥éª¤
"""
import logging
import json
from datetime import datetime
from typing import Dict, Any
from workflows.state import ReportWorkflowState


logger = logging.getLogger(__name__)


def _save_workflow_trace_realtime(state: ReportWorkflowState):
    """
    å®æ—¶ä¿å­˜å·¥ä½œæµè½¨è¿¹åˆ°æ•°æ®åº“
    åœ¨æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåè°ƒç”¨ï¼Œå®ç°å®æ—¶æ›´æ–°
    """
    from models.ai_report import AIReport
    from database import db
    
    try:
        task_context = state.get("task_context", {})
        report_id = task_context.get("report_id")
        
        if not report_id:
            return
        
        report = AIReport.query.get(report_id)
        if report:
            # ä¿å­˜å·¥ä½œæµè½¨è¿¹
            report.execution_path = json.dumps(state.get("execution_path", []), ensure_ascii=False)
            report.workflow_metadata = json.dumps({
                "agent_decisions": state.get("agent_decisions", []),
                "quality_score": state.get("quality_score"),
                "retry_count": state.get("retry_count", 0),
                "start_time": state.get("start_time"),
                "end_time": state.get("end_time")
            }, ensure_ascii=False)
            db.session.commit()
            
            node_count = len(state.get('execution_path', []))
            last_node = state['execution_path'][-1]['node'] if state.get('execution_path') else 'unknown'
            logger.info(f"ğŸ’¾ [å®æ—¶ä¿å­˜] æŠ¥å‘ŠID: {report_id} | èŠ‚ç‚¹æ•°: {node_count} | æœ€æ–°èŠ‚ç‚¹: {last_node}")
    except Exception as e:
        logger.warning(f"âš ï¸ [å®æ—¶ä¿å­˜] å¤±è´¥: {str(e)}")
        # ä¸å½±å“ä¸»æµç¨‹


async def init_task_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N1: åˆå§‹åŒ–ä»»åŠ¡èŠ‚ç‚¹
    - è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡
    - åˆå§‹åŒ–å·¥ä½œæµçŠ¶æ€
    - è®°å½•å¼€å§‹æ—¶é—´
    """
    task_context = state["task_context"]
    report_id = task_context.get("report_id")
    
    logger.info(f"ğŸš€ [N1-ä»»åŠ¡åˆå§‹åŒ–] å¼€å§‹ - æŠ¥å‘ŠID: {report_id}")
    
    # æ›´æ–°çŠ¶æ€
    state["start_time"] = datetime.utcnow().isoformat()
    state["retry_count"] = 0
    state["max_retries"] = 3
    state["agent_decisions"] = []
    state["execution_path"] = [{
        "node": "init_task",
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed"
    }]
    
    logger.info(f"âœ… [N1-ä»»åŠ¡åˆå§‹åŒ–] å®Œæˆ")
    
    # å®æ—¶ä¿å­˜
    _save_workflow_trace_realtime(state)
    
    return state


async def collect_data_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N2: æ•°æ®é‡‡é›†èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰
    - æŸ¥è¯¢å›ºå®šèµ„äº§æ•°æ®ï¼ˆå«æŠ˜æ—§æ˜ç»†ï¼‰
    - æŸ¥è¯¢è™šæ‹Ÿèµ„äº§æ•°æ®ï¼ˆå«æ—¶é—´ä»·å€¼åˆ†æï¼‰
    - æŸ¥è¯¢æ”¶å…¥æ•°æ®ï¼ˆå«ROIè®¡ç®—ï¼‰
    - æŸ¥è¯¢åˆ†ç±»å±‚çº§ç»“æ„
    - æ„å»ºç»“æ„åŒ–æ•°æ® + æ™ºèƒ½æ´å¯Ÿ
    """
    from services.zhipu_service import ZhipuAiService
    from models.category import Category
    from datetime import datetime
    
    task_context = state["task_context"]
    user_id = task_context["user_id"]
    start_date = task_context["start_date"]
    end_date = task_context["end_date"]
    
    logger.info(f"ğŸ“Š [N2-æ•°æ®é‡‡é›†å¢å¼º] å¼€å§‹ - ç”¨æˆ·ID: {user_id}, æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
    
    try:
        # ä½¿ç”¨ç°æœ‰çš„æœåŠ¡æ–¹æ³•å‡†å¤‡åŸºç¡€æ•°æ®
        service = ZhipuAiService(api_token="dummy", model="dummy")  # ä»…ç”¨äºæ•°æ®æŸ¥è¯¢
        raw_data = service.prepare_asset_data(user_id, start_date, end_date)
        
        # ã€å¢å¼º1ã€‘è·å–åˆ†ç±»å±‚çº§ç»“æ„
        categories = Category.query.filter_by(user_id=user_id).all()
        category_hierarchy = []
        for cat in categories:
            category_hierarchy.append({
                'id': cat.id,
                'name': cat.name,
                'parent_id': cat.parent_id,
                'level': cat.get_level(),
                'full_path': cat.get_full_path(),
                'project_count': len(cat.projects)
            })
        
        # ã€å¢å¼º2ã€‘è®¡ç®—æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡
        insights = {
            # å›ºå®šèµ„äº§å¥åº·åº¦
            'fixed_asset_health': _calculate_fixed_asset_health(raw_data['fixed_assets']),
            # è™šæ‹Ÿèµ„äº§æ•ˆç‡è¯„çº§
            'virtual_asset_efficiency': _calculate_virtual_efficiency(raw_data['virtual_assets']),
            # æ”¶å…¥è´¨é‡è¯„åˆ†
            'income_quality': _calculate_income_quality(raw_data['fixed_assets']),
            # èµ„äº§é…ç½®å‡è¡¡åº¦
            'allocation_balance': _calculate_allocation_balance(raw_data),
            # åˆ†ç±»å±‚çº§æ•°æ®
            'category_hierarchy': category_hierarchy
        }
        
        state["raw_data"] = raw_data
        
        logger.info(f"âœ… [N2-æ•°æ®é‡‡é›†å¢å¼º] å®Œæˆ")
        logger.info(f"   - å›ºå®šèµ„äº§: {raw_data['fixed_assets']['total_assets']}é¡¹")
        logger.info(f"   - è™šæ‹Ÿèµ„äº§: {raw_data['virtual_assets']['total_projects']}é¡¹")
        logger.info(f"   - åˆ†ç±»å±‚çº§: {len(category_hierarchy)}ä¸ªåˆ†ç±»ï¼Œæœ€æ·±{max([c['level'] for c in category_hierarchy], default=0)}å±‚")
        logger.info(f"   - æ™ºèƒ½æ´å¯Ÿ: å¥åº·åº¦{insights['fixed_asset_health']:.1f}, æ•ˆç‡{insights['virtual_asset_efficiency']:.1f}")
        
        # å°†insightså•ç‹¬å­˜å‚¨ï¼Œä¸ä¿®æ”¹raw_dataç»“æ„
        state["intelligent_insights"] = insights
        
        state["execution_path"].append({
            "node": "collect_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "data_summary": {
                "fixed_assets_count": raw_data['fixed_assets']['total_assets'],
                "virtual_assets_count": raw_data['virtual_assets']['total_projects'],
                "category_count": len(category_hierarchy),
                "insights": insights
            }
        })
        
        # å®æ—¶ä¿å­˜
        _save_workflow_trace_realtime(state)
        
    except Exception as e:
        logger.error(f"âŒ [N2-æ•°æ®é‡‡é›†] å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        state["error_message"] = f"æ•°æ®é‡‡é›†å¤±è´¥: {str(e)}"
        state["execution_path"].append({
            "node": "collect_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


def _calculate_fixed_asset_health(fixed_data):
    """è®¡ç®—å›ºå®šèµ„äº§å¥åº·åº¦ï¼ˆ0-100ï¼‰"""
    if fixed_data['total_assets'] == 0:
        return 50.0  # æ— èµ„äº§é»˜è®¤ä¸­ç­‰
    
    # è€ƒè™‘å› ç´ ï¼šæŠ˜æ—§ç‡ã€æ”¶å…¥ç‡ã€ä½¿ç”¨ç‡
    health_score = 100.0
    
    # æŠ˜æ—§ç‡æƒ©ç½šï¼ˆæŠ˜æ—§ç‡è¶Šé«˜æ‰£åˆ†è¶Šå¤šï¼‰
    depreciation_penalty = min(40, fixed_data['depreciation_rate'] * 0.5)
    health_score -= depreciation_penalty
    
    # æ”¶å…¥ç‡åŠ åˆ†
    if fixed_data['total_current_value'] > 0:
        income_rate = (fixed_data['total_income'] / fixed_data['total_current_value']) * 100
        income_bonus = min(30, income_rate * 3)
        health_score += income_bonus
    
    # ä½¿ç”¨ç‡åŠ åˆ†
    status_stats = fixed_data.get('status_stats', {})
    if fixed_data['total_assets'] > 0:
        in_use = status_stats.get('ä½¿ç”¨ä¸­', 0)
        usage_rate = (in_use / fixed_data['total_assets']) * 100
        usage_bonus = min(20, usage_rate * 0.2)
        health_score += usage_bonus
    
    return round(max(0, min(100, health_score)), 1)


def _calculate_virtual_efficiency(virtual_data):
    """è®¡ç®—è™šæ‹Ÿèµ„äº§æ•ˆç‡ï¼ˆ0-100ï¼‰"""
    if virtual_data['total_projects'] == 0:
        return 50.0
    
    # åŸºäºåˆ©ç”¨ç‡å’Œæµªè´¹ç‡
    efficiency = virtual_data['utilization_rate'] - virtual_data['waste_rate'] * 2
    
    # å³å°†è¿‡æœŸé¡¹ç›®æ‰£åˆ†
    if virtual_data.get('expiring_soon'):
        expiring_count = len(virtual_data['expiring_soon'])
        efficiency -= min(20, expiring_count * 5)
    
    return round(max(0, min(100, efficiency)), 1)


def _calculate_income_quality(fixed_data):
    """è®¡ç®—æ”¶å…¥è´¨é‡ï¼ˆ0-100ï¼‰"""
    if fixed_data['total_current_value'] == 0:
        return 0.0
    
    # ROIä½œä¸ºä¸»è¦æŒ‡æ ‡
    roi = (fixed_data['total_income'] / fixed_data['total_current_value']) * 100
    
    # è½¬æ¢ä¸º0-100åˆ†æ•°
    # 10% ROI = 100åˆ†, 5% ROI = 50åˆ†, 0% ROI = 0åˆ†
    quality = roi * 10
    
    return round(max(0, min(100, quality)), 1)


def _calculate_allocation_balance(raw_data):
    """è®¡ç®—èµ„äº§é…ç½®å‡è¡¡åº¦ï¼ˆ0-100ï¼‰"""
    fixed_value = raw_data['fixed_assets']['total_current_value']
    virtual_value = raw_data['virtual_assets']['total_amount']
    total = fixed_value + virtual_value
    
    if total == 0:
        return 50.0
    
    # ç†æƒ³æ¯”ä¾‹ï¼šå›ºå®šèµ„äº§60-80%ï¼Œè™šæ‹Ÿèµ„äº§20-40%
    fixed_ratio = (fixed_value / total) * 100
    
    if 60 <= fixed_ratio <= 80:
        balance = 100  # å®Œç¾å‡è¡¡
    elif 50 <= fixed_ratio < 60 or 80 < fixed_ratio <= 90:
        balance = 80   # è‰¯å¥½
    elif 40 <= fixed_ratio < 50 or 90 < fixed_ratio <= 95:
        balance = 60   # ä¸€èˆ¬
    else:
        balance = 40   # å¤±è¡¡
    
    return round(balance, 1)


async def compress_data_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N3: æ•°æ®å‹ç¼©èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰
    - å°†ç»“æ„åŒ–æ•°æ®å‹ç¼©ä¸ºç®€æ´æ–‡æœ¬
    - èåˆæ™ºèƒ½æ´å¯ŸæŒ‡æ ‡
    - ä¸ºAIç”Ÿæˆåšå‡†å¤‡
    """
    from services.zhipu_service import ZhipuAiService
    
    logger.info(f"ğŸ—œï¸ [N3-æ•°æ®å‹ç¼©å¢å¼º] å¼€å§‹")
    
    try:
        raw_data = state["raw_data"]
        if not raw_data:
            raise Exception("åŸå§‹æ•°æ®ä¸ºç©º")
        
        service = ZhipuAiService(api_token="dummy", model="dummy")
        compressed_text = service._compress_data_to_text(raw_data)
        
        # ã€å¢å¼ºã€‘æ·»åŠ æ™ºèƒ½æ´å¯Ÿæ‘˜è¦
        insights = state.get("intelligent_insights")
        if insights:
            insights_text = "\n\nã€æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡ã€‘\n"
            insights_text += f"- ğŸŸ¢ å›ºå®šèµ„äº§å¥åº·åº¦: {insights['fixed_asset_health']:.1f}/100\n"
            insights_text += f"- âš¡ è™šæ‹Ÿèµ„äº§æ•ˆç‡: {insights['virtual_asset_efficiency']:.1f}/100\n"
            insights_text += f"- ğŸ’µ æ”¶å…¥è´¨é‡: {insights['income_quality']:.1f}/100\n"
            insights_text += f"- âš–ï¸ èµ„äº§é…ç½®å‡è¡¡åº¦: {insights['allocation_balance']:.1f}/100\n"
            
            # åˆ†ç±»å±‚çº§ä¿¡æ¯
            if 'category_hierarchy' in insights:
                max_level = max([c['level'] for c in insights['category_hierarchy']], default=0)
                top_categories = [c for c in insights['category_hierarchy'] if c['level'] == 0]
                insights_text += f"\n- ğŸ“‚ åˆ†ç±»ç»“æ„: {len(insights['category_hierarchy'])}ä¸ªåˆ†ç±»ï¼Œ{max_level+1}å±‚æ·±åº¦ï¼Œ{len(top_categories)}ä¸ªé¡¶çº§åˆ†ç±»"
                # æ˜¾ç¤ºå‰3ä¸ªæœ€æ´»è·ƒçš„åˆ†ç±»
                sorted_cats = sorted(insights['category_hierarchy'], key=lambda x: x['project_count'], reverse=True)[:3]
                for cat in sorted_cats:
                    insights_text += f"\n  â€¢ {cat['full_path']}: {cat['project_count']}ä¸ªé¡¹ç›®"
            
            compressed_text += insights_text
        
        state["compressed_text"] = compressed_text
        
        logger.info(f"âœ… [N3-æ•°æ®å‹ç¼©å¢å¼º] å®Œæˆ - å‹ç¼©åæ–‡æœ¬é•¿åº¦: {len(compressed_text)} å­—ç¬¦")
        logger.debug(f"å‹ç¼©åæ–‡æœ¬é¢„è§ˆ:\n{compressed_text[:500]}...")
        
        state["execution_path"].append({
            "node": "compress_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "text_length": len(compressed_text),
            "has_insights": insights is not None
        })
        
        # å®æ—¶ä¿å­˜
        _save_workflow_trace_realtime(state)
        
    except Exception as e:
        logger.error(f"âŒ [N3-æ•°æ®å‹ç¼©] å¤±è´¥: {str(e)}")
        state["error_message"] = f"æ•°æ®å‹ç¼©å¤±è´¥: {str(e)}"
        state["execution_path"].append({
            "node": "compress_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


async def agent_decide_comparison_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N4: Agentå†³ç­–-æ˜¯å¦éœ€è¦ä¸ŠæœŸå¯¹æ¯”
    - åˆ†ææŠ¥å‘Šç±»å‹å’Œæ—¶é—´è·¨åº¦
    - å†³å®šæ˜¯å¦æŸ¥è¯¢ä¸ŠæœŸæ•°æ®
    """
    logger.info(f"ğŸ¤– [N4-Agentå†³ç­–] å¼€å§‹ - åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸ŠæœŸå¯¹æ¯”")
    
    task_context = state["task_context"]
    report_type = task_context.get("report_type", "custom")
    
    # å†³ç­–é€»è¾‘ï¼šå‘¨æŠ¥ã€æœˆæŠ¥ã€å¹´æŠ¥éœ€è¦ä¸ŠæœŸå¯¹æ¯”
    need_comparison = report_type in ['weekly', 'monthly', 'yearly']
    
    decision = {
        "node": "agent_decide_comparison",
        "timestamp": datetime.utcnow().isoformat(),
        "decision": {
            "need_comparison": need_comparison,
            "reason": f"æŠ¥å‘Šç±»å‹ä¸º{report_type}ï¼Œ{'éœ€è¦' if need_comparison else 'æ— éœ€'}ä¸ŠæœŸå¯¹æ¯”",
            "next_node": "query_previous_data" if need_comparison else "generate_report"
        }
    }
    
    state["agent_decisions"].append(decision)
    state["execution_path"].append({
        "node": "agent_decide_comparison",
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed",
        "decision": decision["decision"]
    })
    
    logger.info(f"âœ… [N4-Agentå†³ç­–] å®Œæˆ - {'éœ€è¦' if need_comparison else 'æ— éœ€'}ä¸ŠæœŸå¯¹æ¯”")
    
    return state


async def query_previous_data_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N5: æŸ¥è¯¢ä¸ŠæœŸæ•°æ®èŠ‚ç‚¹
    - è®¡ç®—ä¸ŠæœŸæ—¶é—´èŒƒå›´
    - æŸ¥è¯¢ä¸ŠæœŸæ•°æ®
    - ç”Ÿæˆå¯¹æ¯”æ–‡æœ¬
    """
    from services.zhipu_service import ZhipuAiService
    
    logger.info(f"ğŸ“… [N5-æŸ¥è¯¢ä¸ŠæœŸ] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        user_id = task_context["user_id"]
        start_date = task_context["start_date"]
        end_date = task_context["end_date"]
        
        service = ZhipuAiService(api_token="dummy", model="dummy")
        previous_data = service._get_previous_period_data(user_id, start_date, end_date)
        
        if previous_data:
            state["previous_data"] = previous_data
            
            # ç”Ÿæˆå¯¹æ¯”æ–‡æœ¬
            comparison_text = service._generate_comparison_text(state["raw_data"], previous_data)
            state["comparison_text"] = comparison_text
            
            logger.info(f"âœ… [N5-æŸ¥è¯¢ä¸ŠæœŸ] å®Œæˆ - å·²ç”Ÿæˆå¯¹æ¯”åˆ†æ")
        else:
            logger.warning(f"âš ï¸ [N5-æŸ¥è¯¢ä¸ŠæœŸ] æœªæ‰¾åˆ°ä¸ŠæœŸæ•°æ®ï¼Œè·³è¿‡å¯¹æ¯”")
            state["comparison_text"] = ""
        
        state["execution_path"].append({
            "node": "query_previous_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "has_previous_data": previous_data is not None
        })
        
    except Exception as e:
        logger.warning(f"âš ï¸ [N5-æŸ¥è¯¢ä¸ŠæœŸ] å¼‚å¸¸: {str(e)}ï¼Œè·³è¿‡å¯¹æ¯”")
        state["comparison_text"] = ""
        state["execution_path"].append({
            "node": "query_previous_data",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "skipped",
            "error": str(e)
        })
    
    return state


async def ai_preanalysis_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N6: AIé¢„åˆ†æèŠ‚ç‚¹ï¼ˆå®šæ€§åˆ†æé˜¶æ®µï¼‰
    - åŸºäºæ™ºèƒ½æ´å¯ŸæŒ‡æ ‡å’Œå‹ç¼©æ•°æ®è¿›è¡ŒAIå®šæ€§åˆ†æ
    - è¾“å‡ºç»“æ„åŒ–çš„å®šæ€§ç»“è®ºï¼Œä¸ºåç»­æŠ¥å‘Šç”Ÿæˆæä¾›æŒ‡å¯¼
    - è¿™æ˜¯"ä»æ•°æ®åˆ°ç»“è®º"çš„å…³é”®ç¬¬ä¸€æ­¥
    """
    from services.zhipu_service import ZhipuAiService
    import json
    
    logger.info(f"ğŸ§  [N6-AIå®šæ€§åˆ†æ] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        api_key = task_context.get("api_key")
        model = task_context.get("model", "glm-4-flash")
        
        if not api_key:
            logger.warning(f"âš ï¸ [N6-AIå®šæ€§åˆ†æ] API Keyæœªé…ç½®ï¼Œè·³è¿‡é¢„åˆ†æ")
            state["qualitative_analysis"] = None
            state["ai_insights"] = ""  # å…¼å®¹æ—§å­—æ®µ
            state["execution_path"].append({
                "node": "ai_preanalysis",
                "timestamp": datetime.utcnow().isoformat(),
                "status": "skipped",
                "reason": "API Keyæœªé…ç½®"
            })
            return state
        
        # è·å–æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡
        intelligent_insights = state.get("intelligent_insights", {})
        compressed_text = state["compressed_text"]
        
        if not intelligent_insights:
            logger.warning(f"âš ï¸ [N6-AIå®šæ€§åˆ†æ] æœªæ‰¾åˆ°æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡ï¼Œè·³è¿‡é¢„åˆ†æ")
            state["qualitative_analysis"] = None
            state["ai_insights"] = ""
            state["execution_path"].append({
                "node": "ai_preanalysis",
                "timestamp": datetime.utcnow().isoformat(),
                "status": "skipped",
                "reason": "æœªæ‰¾åˆ°æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡"
            })
            return state
        
        # æå–å…³é”®æŒ‡æ ‡
        fixed_asset_health = intelligent_insights.get('fixed_asset_health', 0)
        virtual_asset_efficiency = intelligent_insights.get('virtual_asset_efficiency', 0)
        income_quality = intelligent_insights.get('income_quality', 0)
        allocation_balance = intelligent_insights.get('allocation_balance', 0)
        
        logger.info(f"   - å›ºå®šèµ„äº§å¥åº·åº¦: {fixed_asset_health:.1f}/100")
        logger.info(f"   - è™šæ‹Ÿèµ„äº§æ•ˆç‡: {virtual_asset_efficiency:.1f}/100")
        logger.info(f"   - æ”¶å…¥è´¨é‡: {income_quality:.1f}/100")
        logger.info(f"   - èµ„äº§é…ç½®å‡è¡¡åº¦: {allocation_balance:.1f}/100")
        
        # æ„å»ºå®šæ€§åˆ†æPrompt
        qualitative_prompt = f"""
ã€ä»»åŠ¡ã€‘ä½œä¸ºä¸“ä¸šçš„èµ„äº§ç®¡ç†åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡å’Œæ•°æ®æ‘˜è¦ï¼Œç”Ÿæˆä¸€ä»½**ç»“æ„åŒ–çš„å®šæ€§åˆ†æç»“è®º**ã€‚

ã€æ™ºèƒ½æ´å¯ŸæŒ‡æ ‡ã€‘
- ğŸ  å›ºå®šèµ„äº§å¥åº·åº¦: {fixed_asset_health:.1f}/100
- âš¡ è™šæ‹Ÿèµ„äº§æ•ˆç‡: {virtual_asset_efficiency:.1f}/100
- ğŸ’µ æ”¶å…¥è´¨é‡: {income_quality:.1f}/100
- âš–ï¸ èµ„äº§é…ç½®å‡è¡¡åº¦: {allocation_balance:.1f}/100

ã€æ•°æ®æ‘˜è¦ã€‘
{compressed_text[:2000]}  # åªå–å‰2000å­—ç¬¦é¿å…å¤ªé•¿

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·ä»¥JSONæ ¼å¼è¾“å‡ºä»¥ä¸‹ç»“æ„çš„å®šæ€§åˆ†æç»“è®ºï¼š

```json
{{
  "overall_assessment": "æ•´ä½“è¯„ä¼°ï¼šä¼˜ç§€/è‰¯å¥½/ä¸­ç­‰/è¾ƒå·®/å±é™©",
  "severity_level": "ç´§æ€¥ç¨‹åº¦ï¼šä½/ä¸­/é«˜",
  "key_issues": [
    "é—®é¢˜1ï¼šæè¿°å‘ç°çš„ä¸»è¦é—®é¢˜",
    "é—®é¢˜2ï¼š..."
  ],
  "strengths": [
    "ä¼˜åŠ¿1ï¼šæè¿°èµ„äº§ç®¡ç†çš„äº®ç‚¹",
    "ä¼˜åŠ¿2ï¼š..."
  ],
  "focus_areas": [
    "é‡ç‚¹å…³æ³¨åŒºåŸŸ1",
    "é‡ç‚¹å…³æ³¨åŒºåŸŸ2"
  ],
  "preliminary_recommendations": [
    "åˆæ­¥å»ºè®®1",
    "åˆæ­¥å»ºè®®2"
  ],
  "analysis_summary": "ç”¨200å­—æ€»ç»“ä¸Šè¿°åˆ¤æ–­çš„æ ¸å¿ƒé€»è¾‘"
}}
```

ã€åˆ†æåŸåˆ™ã€‘
1. **æŒ‡æ ‡é—¨é™**: è¯„åˆ†>=80ä¸ºä¼˜ç§€ï¼Œ60-80ä¸ºè‰¯å¥½ï¼Œ40-60ä¸ºä¸­ç­‰ï¼Œ<40ä¸ºéœ€å…³æ³¨
2. **é—®é¢˜è¯†åˆ«**: é‡ç‚¹æŒ‡å‡ºè¯„åˆ†<60çš„æŒ‡æ ‡åŠåŸå› 
3. **ä¼˜åŠ¿å‘ç°**: è¯†åˆ«è¯„åˆ†>=80çš„äº®ç‚¹é¢†åŸŸ
4. **å…³æ³¨é‡ç‚¹**: æ˜ç¡®åç»­æŠ¥å‘Šåº”æ·±å…¥åˆ†æçš„é¢†åŸŸ
5. **åˆæ­¥å»ºè®®**: æä¾›é«˜å±‚æ¬¡çš„æ”¹è¿›æ–¹å‘

è¯·ç›´æ¥è¾“å‡ºçº¯JSONï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡å­—ã€‚
"""
        
        # è°ƒç”¨AIç”Ÿæˆå®šæ€§åˆ†æ
        service = ZhipuAiService(api_token=api_key, model=model)
        
        logger.info(f"ğŸ¤– [N6-AIå®šæ€§åˆ†æ] è°ƒç”¨AIè¿›è¡Œå®šæ€§åˆ†æ...")
        response = service.client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„èµ„äº§ç®¡ç†åˆ†æå¸ˆï¼Œæ“…é•¿åŸºäºé‡åŒ–æŒ‡æ ‡å¿«é€Ÿè¯†åˆ«é—®é¢˜å’Œæœºä¼šã€‚"},
                {"role": "user", "content": qualitative_prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )
        
        qualitative_text = response.choices[0].message.content.strip()
        logger.info(f"âœ… [N6-AIå®šæ€§åˆ†æ] AIè¿”å›é•¿åº¦: {len(qualitative_text)} å­—ç¬¦")
        
        # è§£æJSONç»“æœ
        try:
            # æå–JSONï¼ˆå»é™¤ä»£ç å—æ ‡è®°ï¼‰
            if "```json" in qualitative_text:
                qualitative_text = qualitative_text.split("```json")[1].split("```")[0].strip()
            elif "```" in qualitative_text:
                qualitative_text = qualitative_text.split("```")[1].split("```")[0].strip()
            
            qualitative_analysis = json.loads(qualitative_text)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ["overall_assessment", "severity_level", "key_issues", "focus_areas"]
            for field in required_fields:
                if field not in qualitative_analysis:
                    logger.warning(f"âš ï¸ [N6-AIå®šæ€§åˆ†æ] ç¼ºå°‘å¿…è¦å­—æ®µ: {field}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    qualitative_analysis[field] = "" if field in ["overall_assessment", "severity_level"] else []
            
            state["qualitative_analysis"] = qualitative_analysis
            state["ai_insights"] = qualitative_text  # ä¿ç•™åŸå§‹æ–‡æœ¬ä¾›å…¼å®¹
            
            logger.info(f"âœ… [N6-AIå®šæ€§åˆ†æ] å®Œæˆ")
            logger.info(f"   - æ•´ä½“è¯„ä¼°: {qualitative_analysis['overall_assessment']}")
            logger.info(f"   - ç´§æ€¥ç¨‹åº¦: {qualitative_analysis['severity_level']}")
            logger.info(f"   - å…³é”®é—®é¢˜æ•°: {len(qualitative_analysis.get('key_issues', []))}")
            logger.info(f"   - é‡ç‚¹å…³æ³¨: {', '.join(qualitative_analysis.get('focus_areas', [])[:3])}")
            
            state["execution_path"].append({
                "node": "ai_preanalysis",
                "timestamp": datetime.utcnow().isoformat(),
                "status": "completed",
                "qualitative_summary": {
                    "assessment": qualitative_analysis['overall_assessment'],
                    "severity": qualitative_analysis['severity_level'],
                    "issues_count": len(qualitative_analysis.get('key_issues', [])),
                    "focus_areas": qualitative_analysis.get('focus_areas', [])
                }
            })
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [N6-AIå®šæ€§åˆ†æ] JSONè§£æå¤±è´¥: {str(e)}")
            logger.error(f"AIè¿”å›å†…å®¹: {qualitative_text[:500]}...")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨åŸºäºæŒ‡æ ‡çš„ç®€å•åˆ¤æ–­
            qualitative_analysis = _generate_simple_qualitative_analysis(intelligent_insights)
            state["qualitative_analysis"] = qualitative_analysis
            state["ai_insights"] = json.dumps(qualitative_analysis, ensure_ascii=False)
            
            state["execution_path"].append({
                "node": "ai_preanalysis",
                "timestamp": datetime.utcnow().isoformat(),
                "status": "completed",
                "note": "AIè§£æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™ç”Ÿæˆ"
            })
        
    except Exception as e:
        logger.error(f"âŒ [N6-AIå®šæ€§åˆ†æ] å¼‚å¸¸: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # é™çº§å¤„ç†
        intelligent_insights = state.get("intelligent_insights", {})
        if intelligent_insights:
            qualitative_analysis = _generate_simple_qualitative_analysis(intelligent_insights)
            state["qualitative_analysis"] = qualitative_analysis
            state["ai_insights"] = json.dumps(qualitative_analysis, ensure_ascii=False)
        else:
            state["qualitative_analysis"] = None
            state["ai_insights"] = ""
        
        state["execution_path"].append({
            "node": "ai_preanalysis",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


def _generate_simple_qualitative_analysis(intelligent_insights: Dict[str, Any]) -> Dict[str, Any]:
    """
    åŸºäºæ™ºèƒ½æ´å¯ŸæŒ‡æ ‡ç”Ÿæˆç®€å•çš„å®šæ€§åˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰
    """
    fixed_asset_health = intelligent_insights.get('fixed_asset_health', 0)
    virtual_asset_efficiency = intelligent_insights.get('virtual_asset_efficiency', 0)
    income_quality = intelligent_insights.get('income_quality', 0)
    allocation_balance = intelligent_insights.get('allocation_balance', 0)
    
    avg_score = (fixed_asset_health + virtual_asset_efficiency + income_quality + allocation_balance) / 4
    
    # æ•´ä½“è¯„ä¼°
    if avg_score >= 80:
        overall_assessment = "ä¼˜ç§€"
        severity_level = "ä½"
    elif avg_score >= 60:
        overall_assessment = "è‰¯å¥½"
        severity_level = "ä½"
    elif avg_score >= 40:
        overall_assessment = "ä¸­ç­‰"
        severity_level = "ä¸­"
    else:
        overall_assessment = "è¾ƒå·®"
        severity_level = "é«˜"
    
    # è¯†åˆ«é—®é¢˜
    key_issues = []
    strengths = []
    focus_areas = []
    
    if fixed_asset_health < 60:
        key_issues.append(f"å›ºå®šèµ„äº§å¥åº·åº¦åä½ï¼ˆ{fixed_asset_health:.1f}åˆ†ï¼‰ï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦æŠ˜æ—§æˆ–æ”¶å…¥ä¸è¶³é—®é¢˜")
        focus_areas.append("å›ºå®šèµ„äº§å¥åº·åº¦æå‡")
    elif fixed_asset_health >= 80:
        strengths.append(f"å›ºå®šèµ„äº§å¥åº·åº¦ä¼˜ç§€ï¼ˆ{fixed_asset_health:.1f}åˆ†ï¼‰")
    
    if virtual_asset_efficiency < 60:
        key_issues.append(f"è™šæ‹Ÿèµ„äº§æ•ˆç‡åä½ï¼ˆ{virtual_asset_efficiency:.1f}åˆ†ï¼‰ï¼Œå­˜åœ¨æµªè´¹æˆ–å³å°†è¿‡æœŸæƒ…å†µ")
        focus_areas.append("è™šæ‹Ÿèµ„äº§åˆ©ç”¨ç‡ä¼˜åŒ–")
    elif virtual_asset_efficiency >= 80:
        strengths.append(f"è™šæ‹Ÿèµ„äº§æ•ˆç‡ä¼˜ç§€ï¼ˆ{virtual_asset_efficiency:.1f}åˆ†ï¼‰")
    
    if income_quality < 60:
        key_issues.append(f"æ”¶å…¥è´¨é‡éœ€æå‡ï¼ˆ{income_quality:.1f}åˆ†ï¼‰ï¼ŒROIè¡¨ç°ä¸ä½³")
        focus_areas.append("å¢åŠ èµ„äº§æ”¶å…¥")
    elif income_quality >= 80:
        strengths.append(f"æ”¶å…¥è´¨é‡ä¼˜ç§€ï¼ˆ{income_quality:.1f}åˆ†ï¼‰")
    
    if allocation_balance < 60:
        key_issues.append(f"èµ„äº§é…ç½®å¤±è¡¡ï¼ˆ{allocation_balance:.1f}åˆ†ï¼‰ï¼Œéœ€è°ƒæ•´ç»“æ„")
        focus_areas.append("èµ„äº§é…ç½®ä¼˜åŒ–")
    elif allocation_balance >= 80:
        strengths.append(f"èµ„äº§é…ç½®å‡è¡¡ï¼ˆ{allocation_balance:.1f}åˆ†ï¼‰")
    
    # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œæ·»åŠ é»˜è®¤å…³æ³¨
    if not focus_areas:
        focus_areas = ["ä¿æŒå½“å‰ä¼˜ç§€çŠ¶æ€", "å¯†åˆ‡ç›‘æ§èµ„äº§å˜åŒ–"]
    
    return {
        "overall_assessment": overall_assessment,
        "severity_level": severity_level,
        "key_issues": key_issues if key_issues else ["æ— æ˜æ˜¾é—®é¢˜"],
        "strengths": strengths if strengths else ["æ— ç‰¹åˆ«çªå‡ºä¹‹å¤„"],
        "focus_areas": focus_areas,
        "preliminary_recommendations": [
            "æ ¹æ®å…³é”®é—®é¢˜åˆ¶å®šé’ˆå¯¹æ€§æ”¹è¿›æ–¹æ¡ˆ",
            "å®šæœŸç›‘æ§å„é¡¹æŒ‡æ ‡å˜åŒ–è¶‹åŠ¿"
        ],
        "analysis_summary": f"æ ¹æ®æ™ºèƒ½æŒ‡æ ‡åˆ†æï¼Œå½“å‰èµ„äº§çŠ¶å†µä¸º{overall_assessment}ï¼ˆå¹³å‡åˆ†{avg_score:.1f}ï¼‰ã€‚" + 
                           ("éœ€é‡ç‚¹å…³æ³¨" + ",".join(focus_areas[:2]) if key_issues else "æ•´ä½“è¡¨ç°è‰¯å¥½")
    }


async def generate_report_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N7: ç”ŸæˆæŠ¥å‘ŠèŠ‚ç‚¹ï¼ˆåŸºäºå®šæ€§åˆ†æçš„å®šé‡æŠ¥å‘Šï¼‰
    - åŸºäºå®šæ€§åˆ†æç»“è®ºè°ƒç”¨AIç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    - è§£æJSONæ ¼å¼å†…å®¹
    - å°†å®šæ€§ç»“è®ºæ³¨å…¥æŠ¥å‘Šï¼Œå½¢æˆå®Œæ•´åˆ†æé“¾
    """
    from services.zhipu_service import ZhipuAiService
    import json
    
    logger.info(f"ğŸ“ [N7-ç”ŸæˆæŠ¥å‘Š] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        api_key = task_context["api_key"]
        model = task_context.get("model", "glm-4-flash")
        report_type = task_context.get("report_type", "custom")
        
        # è·å–å®šæ€§åˆ†æç»“è®º
        qualitative_analysis = state.get("qualitative_analysis")
        intelligent_insights = state.get("intelligent_insights")
        
        if qualitative_analysis:
            logger.info(f"ğŸ¯ [N7-ç”ŸæˆæŠ¥å‘Š] åˆ©ç”¨å®šæ€§åˆ†æç»“è®ºæŒ‡å¯¼æŠ¥å‘Šç”Ÿæˆ")
            logger.info(f"   - æ•´ä½“è¯„ä¼°: {qualitative_analysis.get('overall_assessment')}")
            logger.info(f"   - å…³é”®é—®é¢˜: {len(qualitative_analysis.get('key_issues', []))}ä¸ª")
            logger.info(f"   - é‡ç‚¹å…³æ³¨: {', '.join(qualitative_analysis.get('focus_areas', [])[:2])}")
        else:
            logger.warning(f"âš ï¸ [N7-ç”ŸæˆæŠ¥å‘Š] æœªæ‰¾åˆ°å®šæ€§åˆ†æï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼ç”ŸæˆæŠ¥å‘Š")
        
        service = ZhipuAiService(api_token=api_key, model=model)
        
        # æ ¹æ®æŠ¥å‘Šç±»å‹è°ƒç”¨å¯¹åº”çš„ç”Ÿæˆæ–¹æ³•
        user_id = task_context["user_id"]
        start_date = task_context["start_date"]
        end_date = task_context["end_date"]
        focus_areas = task_context.get("focus_areas", [])
        
        # ã€å¢å¼ºã€‘å°†å®šæ€§åˆ†æå’Œæ™ºèƒ½æ´å¯Ÿä¼ é€’ç»™æŠ¥å‘Šç”Ÿæˆæ–¹æ³•
        if report_type == 'weekly':
            content = service.generate_weekly_report(
                user_id, start_date, end_date,
                qualitative_analysis=qualitative_analysis,
                intelligent_insights=intelligent_insights
            )
        elif report_type == 'monthly':
            content = service.generate_monthly_report(
                user_id, start_date, end_date,
                qualitative_analysis=qualitative_analysis,
                intelligent_insights=intelligent_insights
            )
        elif report_type == 'yearly':
            content = service.generate_custom_report(
                user_id, start_date, end_date, 
                focus_areas or ['å¹´åº¦èµ„äº§å¢é•¿è¶‹åŠ¿', 'å¹´åº¦æ”¶ç›Šè¡¨ç°', 'èµ„äº§é…ç½®ä¼˜åŒ–'],
                qualitative_analysis=qualitative_analysis,
                intelligent_insights=intelligent_insights
            )
        else:  # custom
            content = service.generate_custom_report(
                user_id, start_date, end_date, focus_areas,
                qualitative_analysis=qualitative_analysis,
                intelligent_insights=intelligent_insights
            )
        
        # ã€å¢å¼ºã€‘å°†å®šæ€§åˆ†ææ³¨å…¥æŠ¥å‘Šå†…å®¹
        if qualitative_analysis and content:
            try:
                report_data = json.loads(content)
                # æ·»åŠ å®šæ€§åˆ†æåˆ°æŠ¥å‘Šä¸­
                report_data['qualitative_analysis'] = qualitative_analysis
                content = json.dumps(report_data, ensure_ascii=False, indent=2)
                logger.info(f"âœ… [N7-ç”ŸæˆæŠ¥å‘Š] å·²å°†å®šæ€§åˆ†ææ³¨å…¥æŠ¥å‘Š")
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ [N7-ç”ŸæˆæŠ¥å‘Š] æŠ¥å‘Šå†…å®¹éJSONæ ¼å¼ï¼Œè·³è¿‡æ³¨å…¥")
        
        state["report_content"] = content
        
        logger.info(f"âœ… [N7-ç”ŸæˆæŠ¥å‘Š] å®Œæˆ - å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        state["execution_path"].append({
            "node": "generate_report",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "content_length": len(content),
            "used_qualitative_analysis": qualitative_analysis is not None
        })
        
    except Exception as e:
        logger.error(f"âŒ [N7-ç”ŸæˆæŠ¥å‘Š] å¤±è´¥: {str(e)}")
        state["error_message"] = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
        state["execution_path"].append({
            "node": "generate_report",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


async def evaluate_quality_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N8: è´¨é‡è¯„ä¼°èŠ‚ç‚¹
    - æ£€æŸ¥JSONæ ¼å¼
    - è¯„ä¼°å†…å®¹å®Œæ•´æ€§
    - è¯„ä¼°æ•°æ®å‡†ç¡®æ€§
    - è®¡ç®—è´¨é‡åˆ†æ•°
    """
    import json
    
    logger.info(f"ğŸ” [N8-è´¨é‡è¯„ä¼°] å¼€å§‹")
    
    try:
        report_content = state["report_content"]
        
        if not report_content:
            raise Exception("æŠ¥å‘Šå†…å®¹ä¸ºç©º")
        
        # å°è¯•è§£æJSON
        try:
            content_json = json.loads(report_content)
        except json.JSONDecodeError as e:
            raise Exception(f"JSONè§£æå¤±è´¥: {str(e)}")
        
        # è¯„ä¼°ç»´åº¦
        score = {
            "json_validity": 100,  # JSONæ ¼å¼æœ‰æ•ˆ
            "completeness": 0,  # å†…å®¹å®Œæ•´æ€§
            "data_accuracy": 0,  # æ•°æ®å‡†ç¡®æ€§
            "total_score": 0
        }
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = [
            'executive_summary', 'key_conclusions', 'fixed_asset_analysis',
            'virtual_asset_analysis', 'income_performance', 'asset_allocation_review',
            'actionable_recommendations', 'risk_alerts', 'health_score', 'chart_data'
        ]
        
        present_fields = sum(1 for field in required_fields if field in content_json)
        score["completeness"] = int((present_fields / len(required_fields)) * 100)
        
        # æ£€æŸ¥æ•°æ®å¼•ç”¨ï¼ˆç®€å•æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å€¼ï¼‰
        content_str = str(content_json)
        has_numbers = any(char.isdigit() for char in content_str)
        score["data_accuracy"] = 100 if has_numbers else 0
        
        # è®¡ç®—æ€»åˆ†
        score["total_score"] = int(
            score["json_validity"] * 0.3 +
            score["completeness"] * 0.4 +
            score["data_accuracy"] * 0.3
        )
        
        state["quality_score"] = score
        
        # åˆ¤æ–­æ˜¯å¦åˆæ ¼ï¼ˆæ€»åˆ†â‰¥80ï¼‰
        if score["total_score"] >= 60:
            state["evaluation_result"] = "pass"
            logger.info(f"âœ… [N8-è´¨é‡è¯„ä¼°] å®Œæˆ - è¯„åˆ†: {score['total_score']:.1f}/100ï¼Œåˆæ ¼")
        elif state["retry_count"] < state["max_retries"]:
            state["evaluation_result"] = "retry"
            logger.warning(f"âš ï¸ [N8-è´¨é‡è¯„ä¼°] å®Œæˆ - è¯„åˆ†: {score['total_score']:.1f}/100ï¼Œéœ€è¦é‡è¯•")
        else:
            state["evaluation_result"] = "fail"
            logger.error(f"âŒ [N8-è´¨é‡è¯„ä¼°] å®Œæˆ - è¯„åˆ†: {score['total_score']:.1f}/100ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
        
        state["execution_path"].append({
            "node": "evaluate_quality",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed",
            "quality_score": score,
            "evaluation_result": state["evaluation_result"]
        })
        
    except Exception as e:
        logger.error(f"âŒ [N8-è´¨é‡è¯„ä¼°] å¤±è´¥: {str(e)}")
        state["evaluation_result"] = "retry" if state["retry_count"] < state["max_retries"] else "fail"
        state["execution_path"].append({
            "node": "evaluate_quality",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e),
            "evaluation_result": state["evaluation_result"]
        })
    
    return state


async def save_report_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N9: ä¿å­˜æŠ¥å‘ŠèŠ‚ç‚¹
    - è§£ææŠ¥å‘Šå†…å®¹æå–æ‘˜è¦
    - æ›´æ–°æ•°æ®åº“æŠ¥å‘ŠçŠ¶æ€
    """
    import json
    from models.ai_report import AIReport
    from database import db
    
    logger.info(f"ğŸ’¾ [N9-ä¿å­˜æŠ¥å‘Š] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        report_id = task_context["report_id"]
        
        report = AIReport.query.get(report_id)
        if not report:
            raise Exception(f"æŠ¥å‘Šä¸å­˜åœ¨: {report_id}")
        
        # æå–æ‘˜è¦
        content = state["report_content"]
        if not content:
            raise Exception("æŠ¥å‘Šå†…å®¹ä¸ºç©º")
        
        try:
            content_json = json.loads(content)
            # æ–°æ ¼å¼ï¼šexecutive_summary æ˜¯å¯¹è±¡
            if 'executive_summary' in content_json:
                exec_summary = content_json['executive_summary']
                if isinstance(exec_summary, dict):
                    summary = exec_summary.get('content', exec_summary.get('title', 'æŠ¥å‘Šå·²ç”Ÿæˆ'))
                else:
                    summary = str(exec_summary)
            # æ—§æ ¼å¼ï¼šperiod_summary æ˜¯å­—ç¬¦ä¸²
            elif 'period_summary' in content_json:
                summary = str(content_json['period_summary'])
            else:
                summary = "æŠ¥å‘Šå·²ç”Ÿæˆ"
            
            summary = str(summary)[:500] if summary else "æŠ¥å‘Šå·²ç”Ÿæˆ"
        except Exception as e:
            logger.warning(f"âš ï¸ æ‘˜è¦æå–å¤±è´¥: {e}")
            summary = "æŠ¥å‘Šå·²ç”Ÿæˆ"
        
        # ã€å¢å¼ºã€‘æ·»åŠ æ™ºèƒ½æ´å¯Ÿåˆ°æŠ¥å‘Šå†…å®¹
        intelligent_insights = state.get("intelligent_insights")
        if intelligent_insights:
            # å¦‚æœæ˜¯Markdownæ ¼å¼ï¼Œæ³¨å…¥æ™ºèƒ½æ´å¯Ÿ
            try:
                report_data = json.loads(content)
                if isinstance(report_data, dict):
                    report_data["intelligent_insights"] = intelligent_insights
                    content = json.dumps(report_data, ensure_ascii=False)
                    logger.info(f"âœ… å·²æ³¨å…¥æ™ºèƒ½æ´å¯Ÿåˆ°æŠ¥å‘Šå†…å®¹")
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä¿æŒåŸæ ·
                pass
        
        # æ›´æ–°æŠ¥å‘Š
        report.content = content
        report.summary = summary
        report.status = 'completed'
        report.generated_at = datetime.utcnow()
        
        # æŒä¹…åŒ–å·¥ä½œæµè½¨è¿¹ï¼ˆæ–°å¢ï¼‰
        report.execution_path = json.dumps(state.get("execution_path", []), ensure_ascii=False)
        report.workflow_metadata = json.dumps({
            "agent_decisions": state.get("agent_decisions", []),
            "quality_score": state.get("quality_score"),
            "retry_count": state.get("retry_count", 0),
            "start_time": state.get("start_time"),
            "end_time": state.get("end_time")
        }, ensure_ascii=False)
        
        db.session.commit()
        
        state["end_time"] = datetime.utcnow().isoformat()
        
        logger.info(f"âœ… [N9-ä¿å­˜æŠ¥å‘Š] å®Œæˆ - æŠ¥å‘ŠID: {report_id}")
        
        state["execution_path"].append({
            "node": "save_report",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed"
        })
        
    except Exception as e:
        logger.error(f"âŒ [N9-ä¿å­˜æŠ¥å‘Š] å¤±è´¥: {str(e)}")
        state["error_message"] = f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}"
        state["execution_path"].append({
            "node": "save_report",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "failed",
            "error": str(e)
        })
    
    return state


async def handle_retry_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N10: é‡è¯•å¤„ç†èŠ‚ç‚¹
    - å¢åŠ é‡è¯•è®¡æ•°
    - åˆ†æå¤±è´¥åŸå› 
    - è°ƒæ•´ç”Ÿæˆç­–ç•¥
    """
    logger.info(f"ğŸ”„ [N10-é‡è¯•å¤„ç†] å¼€å§‹")
    
    state["retry_count"] += 1
    current_retry = state["retry_count"]
    max_retries = state["max_retries"]
    
    logger.info(f"âš ï¸ [N10-é‡è¯•å¤„ç†] ç¬¬ {current_retry}/{max_retries} æ¬¡é‡è¯•")
    
    # è®°å½•é‡è¯•å†³ç­–
    decision = {
        "node": "handle_retry",
        "timestamp": datetime.utcnow().isoformat(),
        "decision": {
            "retry_count": current_retry,
            "max_retries": max_retries,
            "reason": "è´¨é‡è¯„ä¼°æœªé€šè¿‡ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ",
            "next_node": "generate_report"
        }
    }
    
    state["agent_decisions"].append(decision)
    state["execution_path"].append({
        "node": "handle_retry",
        "timestamp": datetime.utcnow().isoformat(),
        "status": "completed",
        "retry_count": current_retry
    })
    
    logger.info(f"âœ… [N10-é‡è¯•å¤„ç†] å®Œæˆ - å‡†å¤‡é‡æ–°ç”ŸæˆæŠ¥å‘Š")
    
    return state


async def handle_failure_node(state: ReportWorkflowState) -> ReportWorkflowState:
    """
    N11: å¤±è´¥å¤„ç†èŠ‚ç‚¹
    - è®°å½•å¤±è´¥åŸå› 
    - æ›´æ–°æŠ¥å‘ŠçŠ¶æ€ä¸ºå¤±è´¥
    - ä¿å­˜å·¥ä½œæµè½¨è¿¹
    """
    import json
    from models.ai_report import AIReport
    from database import db
    
    logger.error(f"âŒ [N11-å¤±è´¥å¤„ç†] å¼€å§‹")
    
    try:
        task_context = state["task_context"]
        report_id = task_context["report_id"]
        
        report = AIReport.query.get(report_id)
        if report:
            # ç¡®ä¿error_messageå­˜åœ¨
            if not state.get("error_message"):
                state["error_message"] = "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°"
            
            error_msg = state["error_message"]
            report.status = 'failed'
            report.error_message = error_msg
            
            # ä¿å­˜å·¥ä½œæµè½¨è¿¹ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè¦ä¿å­˜ï¼‰
            report.execution_path = json.dumps(state.get("execution_path", []), ensure_ascii=False)
            report.workflow_metadata = json.dumps({
                "agent_decisions": state.get("agent_decisions", []),
                "quality_score": state.get("quality_score"),
                "retry_count": state.get("retry_count", 0),
                "start_time": state.get("start_time"),
                "end_time": state.get("end_time"),
                "error_message": error_msg
            }, ensure_ascii=False)
            
            db.session.commit()
            
            logger.error(f"âŒ [N11-å¤±è´¥å¤„ç†] å®Œæˆ - æŠ¥å‘ŠID: {report_id}, åŸå› : {error_msg}")
        
        state["end_time"] = datetime.utcnow().isoformat()
        state["execution_path"].append({
            "node": "handle_failure",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "completed"
        })
        
    except Exception as e:
        logger.error(f"âŒ [N11-å¤±è´¥å¤„ç†] å¼‚å¸¸: {str(e)}")
        state["execution_path"].append({
            "node": "handle_failure",
            "timestamp": datetime.utcnow().isoformat(),
            "status": "error",
            "error": str(e)
        })
    
    return state
